{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2e6f67",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Resource Usage Classification for Server Monitoring\n",
    "\n",
    "## 1. Setup and Data Preparation\n",
    "\n",
    " Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60f4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b4ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataQualityReport:\n",
    "    \"\"\"Data quality assessment results\"\"\"\n",
    "    constant_features: List[str]\n",
    "    high_missing_features: List[str]\n",
    "    highly_correlated_pairs: List[Tuple[str, str, float]]\n",
    "    outlier_percentage: Dict[str, float]\n",
    "    temporal_gaps: List[str]\n",
    "    summary_stats: Dict[str, Any]\n",
    "\n",
    "class AdaptiveDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Layer 1: Foundation Data Preprocessing for Multi-Modal System Metrics\n",
    "\n",
    "    Handles vmstat, iostat, netstat, and process metrics with domain-aware\n",
    "    feature engineering and quality assurance for anomaly detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 sequence_length: int = 50,\n",
    "                 sampling_rate: float = 0.2,\n",
    "                 correlation_threshold: float = 0.95,\n",
    "                 missing_threshold: float = 0.1,\n",
    "                 variance_threshold: float = 0.01):\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.variance_threshold = variance_threshold\n",
    "\n",
    "        # Feature definitions from your data\n",
    "        self.feature_groups = {\n",
    "            'vmstat': ['r', 'b', 'avm', 'fre', 'pi', 'po', 'fr', 'interface_in', 'cs', 'us', 'sy', 'idle'],\n",
    "            'iostat': ['tps', 'kb_read', 'kb_wrtn', 'service_time', 'avg_queue_size', 'avg_wait_time'],\n",
    "            'netstat': ['ipkts', 'opkts', 'ierrs', 'oerrs', 'ipkts_rate', 'opkts_rate', 'ierrs_rate', 'oerrs_rate'],\n",
    "            'process': ['cpu', 'mem', 'command']\n",
    "        }\n",
    "\n",
    "        # Domain knowledge for feature importance\n",
    "        self.critical_features = {\n",
    "            'vmstat': ['us', 'sy', 'idle', 'r', 'b'],  # CPU and load\n",
    "            'iostat': ['tps', 'service_time', 'avg_queue_size'],  # I/O performance\n",
    "            'netstat': ['ipkts_rate', 'opkts_rate'],  # Network throughput\n",
    "            'process': ['cpu', 'mem']  # Process resources\n",
    "        }\n",
    "\n",
    "        self.scalers = {}\n",
    "        self.selected_features = {}\n",
    "        self.quality_report = None\n",
    "        self.logger = self._setup_logger()\n",
    "\n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        \"\"\"Setup logging for preprocessing pipeline\"\"\"\n",
    "        logger = logging.getLogger('DataPreprocessor')\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "    def load_and_sample_data(self, file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load and sample data from multiple CSV files efficiently\n",
    "\n",
    "        Args:\n",
    "            file_paths: Dictionary mapping data type to file path\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of sampled DataFrames\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "\n",
    "        for data_type, file_path in file_paths.items():\n",
    "            self.logger.info(f\"üìÇ Processing {data_type} from {file_path}\")\n",
    "\n",
    "            # Use Polars for faster loading, then convert to pandas\n",
    "            df_pl = pl.read_csv(file_path)\n",
    "            total_rows = df_pl.height\n",
    "\n",
    "            # Sample data\n",
    "            if self.sampling_rate < 1.0:\n",
    "                sample_size = int(total_rows * self.sampling_rate)\n",
    "                df_pl = df_pl.sample(n=sample_size, seed=42)\n",
    "                self.logger.info(f\"‚úÖ Loaded {sample_size:,} rows from {total_rows:,} total ({self.sampling_rate*100:.1f}%)\")\n",
    "\n",
    "            # Convert to pandas for compatibility\n",
    "            df = df_pl.to_pandas()\n",
    "\n",
    "            # Basic timestamp validation\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "                invalid_timestamps = df['timestamp'].isna().sum()\n",
    "                if invalid_timestamps > 0:\n",
    "                    self.logger.warning(f\"‚ö†Ô∏è Found {invalid_timestamps} invalid timestamps\")\n",
    "                    df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "            data[data_type] = df\n",
    "\n",
    "        return data\n",
    "\n",
    "    def assess_data_quality(self, data: Dict[str, pd.DataFrame]) -> DataQualityReport:\n",
    "        \"\"\"\n",
    "        Comprehensive data quality assessment\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary of DataFrames by data type\n",
    "\n",
    "        Returns:\n",
    "            DataQualityReport with quality metrics\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üîç Assessing data quality...\")\n",
    "\n",
    "        constant_features = []\n",
    "        high_missing_features = []\n",
    "        highly_correlated_pairs = []\n",
    "        outlier_percentage = {}\n",
    "        temporal_gaps = []\n",
    "        summary_stats = {}\n",
    "\n",
    "        for data_type, df in data.items():\n",
    "            # Get numeric columns only\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if 'timestamp' in numeric_cols:\n",
    "                numeric_cols.remove('timestamp')\n",
    "\n",
    "            # Check for constant values (variance near zero)\n",
    "            for col in numeric_cols:\n",
    "                if df[col].var() < self.variance_threshold:\n",
    "                    constant_features.append(f\"{data_type}.{col}\")\n",
    "                    self.logger.warning(f\"‚ö†Ô∏è {data_type}.{col}: constant values (var={df[col].var():.6f})\")\n",
    "\n",
    "            # Check for high missing values\n",
    "            missing_pct = df[numeric_cols].isnull().mean()\n",
    "            for col, pct in missing_pct.items():\n",
    "                if pct > self.missing_threshold:\n",
    "                    high_missing_features.append(f\"{data_type}.{col}\")\n",
    "                    self.logger.warning(f\"üîß {data_type}.{col}: {pct*100:.1f}% missing\")\n",
    "\n",
    "            # Check for highly correlated features\n",
    "            if len(numeric_cols) > 1:\n",
    "                corr_matrix = df[numeric_cols].corr().abs()\n",
    "                for i in range(len(corr_matrix.columns)):\n",
    "                    for j in range(i+1, len(corr_matrix.columns)):\n",
    "                        if corr_matrix.iloc[i, j] > self.correlation_threshold:\n",
    "                            highly_correlated_pairs.append((\n",
    "                                f\"{data_type}.{corr_matrix.columns[i]}\",\n",
    "                                f\"{data_type}.{corr_matrix.columns[j]}\",\n",
    "                                corr_matrix.iloc[i, j]\n",
    "                            ))\n",
    "\n",
    "            # Outlier detection using IQR\n",
    "            for col in numeric_cols:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "                outlier_percentage[f\"{data_type}.{col}\"] = (outliers / len(df)) * 100\n",
    "\n",
    "            # Temporal gap detection\n",
    "            if 'timestamp' in df.columns:\n",
    "                df_sorted = df.sort_values('timestamp')\n",
    "                time_diffs = df_sorted['timestamp'].diff()\n",
    "                median_interval = time_diffs.median()\n",
    "                large_gaps = (time_diffs > median_interval * 3).sum()\n",
    "                if large_gaps > 0:\n",
    "                    temporal_gaps.append(f\"{data_type}: {large_gaps} gaps > 3x median interval\")\n",
    "\n",
    "            # Summary statistics\n",
    "            summary_stats[data_type] = {\n",
    "                'rows': len(df),\n",
    "                'features': len(numeric_cols),\n",
    "                'missing_data_pct': df[numeric_cols].isnull().mean().mean() * 100,\n",
    "                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "            }\n",
    "\n",
    "        self.quality_report = DataQualityReport(\n",
    "            constant_features=constant_features,\n",
    "            high_missing_features=high_missing_features,\n",
    "            highly_correlated_pairs=highly_correlated_pairs,\n",
    "            outlier_percentage=outlier_percentage,\n",
    "            temporal_gaps=temporal_gaps,\n",
    "            summary_stats=summary_stats\n",
    "        )\n",
    "\n",
    "        return self.quality_report\n",
    "\n",
    "    def engineer_features(self, data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Domain-aware feature engineering for system metrics\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary of raw DataFrames\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of feature-engineered DataFrames\n",
    "        \"\"\"\n",
    "        self.logger.info(\"‚öôÔ∏è Engineering domain-specific features...\")\n",
    "\n",
    "        engineered_data = {}\n",
    "\n",
    "        for data_type, df in data.items():\n",
    "            df_eng = df.copy()\n",
    "\n",
    "            # Add temporal features\n",
    "            if 'timestamp' in df.columns:\n",
    "                df_eng['hour'] = df['timestamp'].dt.hour\n",
    "                df_eng['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "                df_eng['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n",
    "\n",
    "            # Data type specific engineering\n",
    "            if data_type == 'vmstat':\n",
    "                # CPU utilization ratios\n",
    "                if all(col in df.columns for col in ['us', 'sy', 'idle']):\n",
    "                    df_eng['cpu_busy'] = df_eng['us'] + df_eng['sy']\n",
    "                    df_eng['cpu_ratio'] = df_eng['cpu_busy'] / (df_eng['cpu_busy'] + df_eng['idle'] + 1e-6)\n",
    "\n",
    "                # Memory pressure indicators\n",
    "                if all(col in df.columns for col in ['avm', 'fre']):\n",
    "                    df_eng['memory_pressure'] = df_eng['avm'] / (df_eng['avm'] + df_eng['fre'] + 1e-6)\n",
    "\n",
    "                # Load balancing\n",
    "                if 'r' in df.columns and 'b' in df.columns:\n",
    "                    df_eng['total_load'] = df_eng['r'] + df_eng['b']\n",
    "\n",
    "            elif data_type == 'iostat':\n",
    "                # I/O efficiency metrics\n",
    "                if all(col in df.columns for col in ['kb_read', 'kb_wrtn', 'tps']):\n",
    "                    df_eng['io_throughput'] = (df_eng['kb_read'] + df_eng['kb_wrtn']) / (df_eng['tps'] + 1e-6)\n",
    "\n",
    "                # Queue utilization\n",
    "                if 'avg_queue_size' in df.columns and 'service_time' in df.columns:\n",
    "                    df_eng['queue_efficiency'] = df_eng['avg_queue_size'] / (df_eng['service_time'] + 1e-6)\n",
    "\n",
    "            elif data_type == 'netstat':\n",
    "                # Network balance\n",
    "                if 'ipkts_rate' in df.columns and 'opkts_rate' in df.columns:\n",
    "                    df_eng['net_balance'] = (df_eng['ipkts_rate'] - df_eng['opkts_rate']) / (df_eng['ipkts_rate'] + df_eng['opkts_rate'] + 1e-6)\n",
    "                    df_eng['net_total_rate'] = df_eng['ipkts_rate'] + df_eng['opkts_rate']\n",
    "\n",
    "                # Error rates\n",
    "                if all(col in df.columns for col in ['ierrs_rate', 'oerrs_rate', 'ipkts_rate', 'opkts_rate']):\n",
    "                    total_pkts = df_eng['ipkts_rate'] + df_eng['opkts_rate']\n",
    "                    total_errs = df_eng['ierrs_rate'] + df_eng['oerrs_rate']\n",
    "                    df_eng['error_rate'] = total_errs / (total_pkts + 1e-6)\n",
    "\n",
    "            elif data_type == 'process':\n",
    "                # Resource utilization ratios\n",
    "                if 'cpu' in df.columns and 'mem' in df.columns:\n",
    "                    df_eng['resource_ratio'] = df_eng['cpu'] / (df_eng['mem'] + 1e-6)\n",
    "\n",
    "            # Rolling statistics for temporal patterns\n",
    "            numeric_cols = df_eng.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            for col in numeric_cols[:5]:  # Limit to avoid memory explosion\n",
    "                if col not in ['hour', 'day_of_week', 'is_weekend']:\n",
    "                    df_eng[f'{col}_rolling_mean'] = df_eng[col].rolling(window=5, min_periods=1).mean()\n",
    "                    df_eng[f'{col}_rolling_std'] = df_eng[col].rolling(window=5, min_periods=1).std()\n",
    "\n",
    "            engineered_data[data_type] = df_eng\n",
    "\n",
    "        return engineered_data\n",
    "\n",
    "    def select_features(self, data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Intelligent feature selection based on domain knowledge and statistics\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary of feature-engineered DataFrames\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of DataFrames with selected features\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üéØ Selecting optimal features...\")\n",
    "\n",
    "        selected_data = {}\n",
    "\n",
    "        for data_type, df in data.items():\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "            # Start with critical features\n",
    "            selected_features = []\n",
    "            for critical_feature in self.critical_features.get(data_type, []):\n",
    "                if critical_feature in numeric_cols:\n",
    "                    selected_features.append(critical_feature)\n",
    "\n",
    "            # Remove constant features identified in quality assessment\n",
    "            if self.quality_report:\n",
    "                constant_features = [f.split('.')[-1] for f in self.quality_report.constant_features\n",
    "                                   if f.startswith(data_type)]\n",
    "                numeric_cols = [col for col in numeric_cols if col not in constant_features]\n",
    "\n",
    "            # Remove highly correlated features (keep first one)\n",
    "            if self.quality_report:\n",
    "                correlated_features = set()\n",
    "                for feat1, feat2, corr in self.quality_report.highly_correlated_pairs:\n",
    "                    if feat1.startswith(data_type) and feat2.startswith(data_type):\n",
    "                        feat1_name = feat1.split('.')[-1]\n",
    "                        feat2_name = feat2.split('.')[-1]\n",
    "                        if feat1_name in selected_features:\n",
    "                            correlated_features.add(feat2_name)\n",
    "                        else:\n",
    "                            correlated_features.add(feat1_name)\n",
    "                numeric_cols = [col for col in numeric_cols if col not in correlated_features]\n",
    "\n",
    "            # Add engineered features\n",
    "            engineered_features = [col for col in numeric_cols\n",
    "                                 if any(suffix in col for suffix in ['_ratio', '_pressure', '_efficiency', '_balance', '_rate', '_rolling_mean'])]\n",
    "            selected_features.extend(engineered_features[:5])  # Limit engineered features\n",
    "\n",
    "            # Add remaining high-variance features\n",
    "            remaining_features = [col for col in numeric_cols if col not in selected_features]\n",
    "            if remaining_features:\n",
    "                # Calculate variance for remaining features\n",
    "                variances = df[remaining_features].var().sort_values(ascending=False)\n",
    "                high_variance_features = variances.head(10 - len(selected_features)).index.tolist()\n",
    "                selected_features.extend(high_variance_features)\n",
    "\n",
    "            # Ensure we have timestamp and essential columns\n",
    "            essential_cols = ['timestamp'] if 'timestamp' in df.columns else []\n",
    "            if data_type == 'process' and 'command' in df.columns:\n",
    "                essential_cols.append('command')\n",
    "\n",
    "            final_features = essential_cols + selected_features\n",
    "            selected_data[data_type] = df[final_features]\n",
    "\n",
    "            self.selected_features[data_type] = final_features\n",
    "            self.logger.info(f\"‚úÖ {data_type}: Selected {len(final_features)} features from {len(df.columns)} original\")\n",
    "\n",
    "        return selected_data\n",
    "\n",
    "    def normalize_and_scale(self, data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Robust scaling for anomaly detection\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary of DataFrames with selected features\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of scaled DataFrames\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üìè Normalizing and scaling features...\")\n",
    "\n",
    "        scaled_data = {}\n",
    "\n",
    "        for data_type, df in data.items():\n",
    "            df_scaled = df.copy()\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "            if numeric_cols:\n",
    "                # Use RobustScaler for anomaly detection (less sensitive to outliers)\n",
    "                scaler = RobustScaler()\n",
    "                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "                self.scalers[data_type] = scaler\n",
    "\n",
    "            scaled_data[data_type] = df_scaled\n",
    "\n",
    "        return scaled_data\n",
    "\n",
    "    def create_sequences(self, data: Dict[str, pd.DataFrame]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sequences for time series analysis\n",
    "\n",
    "        Args:\n",
    "            data: Dictionary of scaled DataFrames\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of sequence arrays\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"üîó Creating sequences of length {self.sequence_length}...\")\n",
    "\n",
    "        sequences = {}\n",
    "\n",
    "        for data_type, df in data.items():\n",
    "            if 'timestamp' in df.columns:\n",
    "                df_sorted = df.sort_values('timestamp')\n",
    "                numeric_cols = df_sorted.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "                if len(numeric_cols) > 0:\n",
    "                    values = df_sorted[numeric_cols].values\n",
    "\n",
    "                    # Create overlapping sequences\n",
    "                    sequence_list = []\n",
    "                    for i in range(len(values) - self.sequence_length + 1):\n",
    "                        sequence_list.append(values[i:i + self.sequence_length])\n",
    "\n",
    "                    if sequence_list:\n",
    "                        sequences[data_type] = np.array(sequence_list)\n",
    "                        self.logger.info(f\"‚úÖ {data_type}: Created {len(sequence_list)} sequences of shape {sequences[data_type].shape}\")\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def fit_transform(self, file_paths: Dict[str, str]) -> Tuple[Dict[str, np.ndarray], DataQualityReport]:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "\n",
    "        Args:\n",
    "            file_paths: Dictionary mapping data type to file path\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (processed sequences, quality report)\n",
    "        \"\"\"\n",
    "        self.logger.info(\"üöÄ Starting Layer 1 preprocessing pipeline...\")\n",
    "\n",
    "        # Step 1: Load and sample data\n",
    "        data = self.load_and_sample_data(file_paths)\n",
    "\n",
    "        # Step 2: Assess data quality\n",
    "        quality_report = self.assess_data_quality(data)\n",
    "\n",
    "        # Step 3: Feature engineering\n",
    "        engineered_data = self.engineer_features(data)\n",
    "\n",
    "        # Step 4: Feature selection\n",
    "        selected_data = self.select_features(engineered_data)\n",
    "\n",
    "        # Step 5: Normalization and scaling\n",
    "        scaled_data = self.normalize_and_scale(selected_data)\n",
    "\n",
    "        # Step 6: Create sequences\n",
    "        sequences = self.create_sequences(scaled_data)\n",
    "\n",
    "        self.logger.info(\"‚úÖ Layer 1 preprocessing pipeline completed!\")\n",
    "\n",
    "        return sequences, quality_report\n",
    "\n",
    "    def save_preprocessor(self, filepath: str):\n",
    "        \"\"\"Save the fitted preprocessor\"\"\"\n",
    "        joblib.dump({\n",
    "            'scalers': self.scalers,\n",
    "            'selected_features': self.selected_features,\n",
    "            'config': {\n",
    "                'sequence_length': self.sequence_length,\n",
    "                'sampling_rate': self.sampling_rate,\n",
    "                'correlation_threshold': self.correlation_threshold,\n",
    "                'missing_threshold': self.missing_threshold,\n",
    "                'variance_threshold': self.variance_threshold\n",
    "            }\n",
    "        }, filepath)\n",
    "        self.logger.info(f\"üíæ Preprocessor saved to {filepath}\")\n",
    "\n",
    "    def load_preprocessor(self, filepath: str):\n",
    "        \"\"\"Load a fitted preprocessor\"\"\"\n",
    "        loaded = joblib.load(filepath)\n",
    "        self.scalers = loaded['scalers']\n",
    "        self.selected_features = loaded['selected_features']\n",
    "        config = loaded['config']\n",
    "        self.sequence_length = config['sequence_length']\n",
    "        self.sampling_rate = config['sampling_rate']\n",
    "        self.correlation_threshold = config['correlation_threshold']\n",
    "        self.missing_threshold = config['missing_threshold']\n",
    "        self.variance_threshold = config['variance_threshold']\n",
    "        self.logger.info(f\"üìÇ Preprocessor loaded from {filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276c3122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 09:53:31,657 - DataPreprocessor - INFO - üöÄ Starting Layer 1 preprocessing pipeline...\n",
      "2025-07-07 09:53:31,660 - DataPreprocessor - INFO - üìÇ Processing vmstat from D:\\projet\\exports\\vmstat_metric.csv\n",
      "2025-07-07 09:53:32,304 - DataPreprocessor - INFO - ‚úÖ Loaded 250,080 rows from 1,250,404 total (20.0%)\n",
      "2025-07-07 09:53:32,960 - DataPreprocessor - INFO - üìÇ Processing iostat from D:\\projet\\exports\\iostat_metric.csv\n",
      "2025-07-07 09:53:34,528 - DataPreprocessor - INFO - ‚úÖ Loaded 1,126,140 rows from 5,630,702 total (20.0%)\n",
      "2025-07-07 09:53:37,999 - DataPreprocessor - INFO - üìÇ Processing netstat from D:\\projet\\exports\\netstat_metric.csv\n",
      "2025-07-07 09:53:38,746 - DataPreprocessor - INFO - ‚úÖ Loaded 462,362 rows from 2,311,810 total (20.0%)\n",
      "2025-07-07 09:53:39,830 - DataPreprocessor - INFO - üìÇ Processing process from D:\\projet\\exports\\process_metric.csv\n",
      "2025-07-07 09:53:43,768 - DataPreprocessor - INFO - ‚úÖ Loaded 2,108,957 rows from 10,544,788 total (20.0%)\n",
      "2025-07-07 09:53:50,407 - DataPreprocessor - WARNING - ‚ö†Ô∏è Found 1 invalid timestamps\n",
      "2025-07-07 09:53:50,882 - DataPreprocessor - INFO - üîç Assessing data quality...\n",
      "2025-07-07 09:53:50,918 - DataPreprocessor - WARNING - ‚ö†Ô∏è vmstat.b: constant values (var=0.001497)\n",
      "2025-07-07 09:53:50,929 - DataPreprocessor - WARNING - ‚ö†Ô∏è vmstat.pi: constant values (var=0.000004)\n",
      "2025-07-07 09:53:52,819 - DataPreprocessor - WARNING - ‚ö†Ô∏è netstat.ierrs: constant values (var=0.000000)\n",
      "2025-07-07 09:53:52,835 - DataPreprocessor - WARNING - ‚ö†Ô∏è netstat.oerrs: constant values (var=0.000000)\n",
      "2025-07-07 09:53:52,845 - DataPreprocessor - WARNING - ‚ö†Ô∏è netstat.time: constant values (var=0.000000)\n",
      "2025-07-07 09:53:52,879 - DataPreprocessor - WARNING - ‚ö†Ô∏è netstat.ierrs_rate: constant values (var=0.000000)\n",
      "2025-07-07 09:53:52,895 - DataPreprocessor - WARNING - ‚ö†Ô∏è netstat.oerrs_rate: constant values (var=0.000000)\n",
      "2025-07-07 09:53:56,917 - DataPreprocessor - INFO - ‚öôÔ∏è Engineering domain-specific features...\n",
      "2025-07-07 09:54:01,088 - DataPreprocessor - INFO - üéØ Selecting optimal features...\n",
      "2025-07-07 09:54:01,249 - DataPreprocessor - INFO - ‚úÖ vmstat: Selected 11 features from 31 original\n",
      "2025-07-07 09:54:01,857 - DataPreprocessor - INFO - ‚úÖ iostat: Selected 11 features from 21 original\n",
      "2025-07-07 09:54:02,110 - DataPreprocessor - INFO - ‚úÖ netstat: Selected 11 features from 28 original\n",
      "2025-07-07 09:54:03,360 - DataPreprocessor - INFO - ‚úÖ process: Selected 12 features from 19 original\n",
      "2025-07-07 09:54:03,363 - DataPreprocessor - INFO - üìè Normalizing and scaling features...\n",
      "2025-07-07 09:54:07,930 - DataPreprocessor - INFO - üîó Creating sequences of length 50...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 954. MiB for an array with shape (250031, 50, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     11\u001b[39m file_paths = {\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvmstat\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprojet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mexports\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mvmstat_metric.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miostat\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprojet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mexports\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33miostat_metric.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnetstat\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprojet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mexports\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnetstat_metric.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprocess\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprojet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mexports\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprocess_metric.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Run preprocessing pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m sequences, quality_report = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Processing Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 422\u001b[39m, in \u001b[36mAdaptiveDataPreprocessor.fit_transform\u001b[39m\u001b[34m(self, file_paths)\u001b[39m\n\u001b[32m    419\u001b[39m scaled_data = \u001b[38;5;28mself\u001b[39m.normalize_and_scale(selected_data)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# Step 6: Create sequences\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m sequences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Layer 1 preprocessing pipeline completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sequences, quality_report\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 389\u001b[39m, in \u001b[36mAdaptiveDataPreprocessor.create_sequences\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    386\u001b[39m                 sequence_list.append(values[i:i + \u001b[38;5;28mself\u001b[39m.sequence_length])\n\u001b[32m    388\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m sequence_list:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m                 sequences[data_type] = np.array(sequence_list)\n\u001b[32m    390\u001b[39m                 \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sequence_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sequences of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequences[data_type].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 954. MiB for an array with shape (250031, 50, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = AdaptiveDataPreprocessor(\n",
    "        sequence_length=50,\n",
    "        sampling_rate=0.2,\n",
    "        correlation_threshold=0.8\n",
    "    )\n",
    "\n",
    "    # Define file paths (adjust to your actual paths)\n",
    "    file_paths = {\n",
    "        'vmstat': 'D:\\\\projet\\\\exports\\\\vmstat_metric.csv',\n",
    "        'iostat': 'D:\\\\projet\\\\exports\\\\iostat_metric.csv',\n",
    "        'netstat': 'D:\\\\projet\\\\exports\\\\netstat_metric.csv',\n",
    "        'process': 'D:\\\\projet\\\\exports\\\\process_metric.csv'\n",
    "    }\n",
    "\n",
    "    # Run preprocessing pipeline\n",
    "    sequences, quality_report = preprocessor.fit_transform(file_paths)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nüìä Processing Results:\")\n",
    "    for data_type, seq_array in sequences.items():\n",
    "        print(f\"{data_type}: {seq_array.shape[0]} sequences, {seq_array.shape[1]} timesteps, {seq_array.shape[2]} features\")\n",
    "\n",
    "    # Save for later use\n",
    "    preprocessor.save_preprocessor('layer1_preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ac7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
